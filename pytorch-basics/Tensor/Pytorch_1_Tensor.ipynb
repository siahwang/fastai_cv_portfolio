{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "#### array/matrix -like datatype\n",
    "\n",
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generating directly from Python list \n",
    "data=[[1,2],[3,4]]\n",
    "x_data=torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) from NumPy array\n",
    "np_array=np.array(data)\n",
    "x_np= torch.from_numpy(np_array)\n",
    "# Tensor.numpy(): Tensor --> numpy\n",
    "# x_np & np_array share memory: changing one would automatically change the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.8075, 0.9111],\n",
      "        [0.1643, 0.0924]])\n"
     ]
    }
   ],
   "source": [
    "# 3) from existing Tensor \n",
    "x_ones=torch.ones_like(x_data)\n",
    "# This generates a Tensor consisting of 1 that has the same shape & dtype as x_data\n",
    "x_rand =torch.rand_like(x_data, dtype=torch.float)\n",
    "\n",
    "# *_like() --> keeps the form of the original Tensor\n",
    "\n",
    "print(x_ones)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0156, 0.0338, 0.1041],\n",
      "        [0.0937, 0.5726, 0.8000]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 4) random & const. \n",
    "shape = (2,3,)\n",
    "rand_tensor=torch.rand(shape)\n",
    "ones_tensor=torch.ones(shape)\n",
    "zeros_tensor=torch.zeros(shape)\n",
    "\n",
    "print(rand_tensor)\n",
    "print(ones_tensor)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute \n",
    "#### shape, datatype and the device tensor is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.rand(3,4)\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operation\n",
    "#### e.g. transposing, indexing, slicing, calculation, linear algebra, random sampling,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Moving Tensors Between CPU <-> GPU\n",
    "# By default, Tensors are created in CPU memory.\n",
    "# If a GPU is available, it is possible to move Tensors to the GPU using .to(\"cuda\") or .cuda() for significantly faster computation.\n",
    "# However, frequently transferring large Tensors between devices adds overheadâ€”so try to keep them on a single device whenever possible.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor=tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 2. Indexing & Slicing\n",
    "tensor = torch.ones(4,4)\n",
    "\n",
    "# 1st row\n",
    "print(tensor[0])\n",
    "\n",
    "# 1st col\n",
    "print(tensor[:, 0])\n",
    "\n",
    "# last col\n",
    "print(tensor[..., -1])\n",
    "\n",
    "# Changing all the values of 2nd col to zero\n",
    "tensor[:, 1]=0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "# 3. Concatenataion VS. Stacking\n",
    "# torch.cat = concatenating multiple Tensors along pre-existing axis\n",
    "# torch.stack = generating additional axis and stacking Tensors along that axis\n",
    "\n",
    "# (4x4 Tensor --> concatenate three times along dim=1 --> 4X12 Tensor)\n",
    "t1=torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) <class 'torch.Tensor'>\n",
      "12.0 <class 'float'>\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 4. Arithmetic Operations\n",
    "# matrix multplication (y1, y2, y3: hold the same values)\n",
    "# tensor.T --> transpose of tensor\n",
    "y1= tensor @ tensor.T\n",
    "y2= tensor.matmul(tensor.T)\n",
    "\n",
    "y3= torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3) # this could reduce overhead \n",
    "\n",
    "# element-wise product (z1, z2, z3: hold the same values)\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3=torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "\n",
    "# Single-element Tensors \n",
    "# When aggregating a Tensor into a single-element Tensor (e.g., via sum()),\n",
    "# use .item() to convert it to a native Python number\n",
    "agg=tensor.sum()\n",
    "agg_item=agg.item()\n",
    "\n",
    "print(agg, type(agg)) # Tensor\n",
    "print(agg_item, type(agg_item)) #float\n",
    "\n",
    "\n",
    "# In-place operations\n",
    "# Operations that store their result in the operand are called in-place and use a trailing underscore.\n",
    "# For example, x.copy_(y) or x.t_() modify x directly.\n",
    "print(tensor)\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "\n",
    "# Note:\n",
    "# In-place operations save some memory but immediately overwrite the computation history,\n",
    "# which can interfere with automatic differentiation (gradient calculation).\n",
    "# Therefore, their use is generally discouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([4., 4., 4., 4., 4.])\n",
      "[4. 4. 4. 4. 4.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 1) Tensor -> Numpy: Convert a Tensor to a NumPy array (shares the same memory)\n",
    "t=torch.ones(5)\n",
    "print(t)\n",
    "n=t.numpy()\n",
    "print(n)\n",
    "\n",
    "# In-place addition on the Tensor will reflect in the NumPy array\n",
    "t.add_(3)\n",
    "print(t)\n",
    "print(n)\n",
    "\n",
    "# 2) Numpy -> Tensor: Convert a NumPy array to a Tensor (shares the same memory)\n",
    "n= np.ones(5)\n",
    "t= torch.from_numpy(n)\n",
    "\n",
    "print(n)\n",
    "print(t)\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(n)\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
